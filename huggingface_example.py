import requests
from transformers import pipeline, AutoTokenizer, AutoModel
from huggingface_hub import HfApi
from config import config

def setup_huggingface():
    """Hugging Face ì„¤ì •"""
    if not config.HUGGINGFACE_API_KEY:
        print("âš ï¸  Hugging Face API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¼ë¶€ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return False
    
    # API í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •
    import os
    os.environ['HUGGING_FACE_HUB_TOKEN'] = config.HUGGINGFACE_API_KEY
    print("âœ… Hugging Faceê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return True

def get_huggingface_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    try:
        api = HfApi()
        models = api.list_models(limit=5)  # ì²˜ìŒ 5ê°œ ëª¨ë¸ë§Œ ê°€ì ¸ì˜¤ê¸°
        print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤:")
        for model in models:
            print(f"   - {model.modelId}")
        return models
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return []

def text_classification_example(text: str):
    """í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì˜ˆì‹œ"""
    try:
        classifier = pipeline(
            "text-classification",
            model="distilbert-base-uncased-finetuned-sst-2-english",
            token=config.HUGGINGFACE_API_KEY
        )
        result = classifier(text)
        return result
    except Exception as e:
        print(f"âŒ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def sentiment_analysis_example(text: str):
    """ê°ì • ë¶„ì„ ì˜ˆì‹œ"""
    try:
        sentiment_analyzer = pipeline(
            "sentiment-analysis",
            model="cardiffnlp/twitter-roberta-base-sentiment-l14",
            token=config.HUGGINGFACE_API_KEY
        )
        result = sentiment_analyzer(text)
        return result
    except Exception as e:
        print(f"âŒ ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def text_generation_example(prompt: str):
    """í…ìŠ¤íŠ¸ ìƒì„± ì˜ˆì‹œ"""
    try:
        generator = pipeline(
            "text-generation",
            model="gpt2",
            token=config.HUGGINGFACE_API_KEY
        )
        result = generator(prompt, max_length=100, num_return_sequences=1)
        return result
    except Exception as e:
        print(f"âŒ í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def translation_example(text: str, source_lang: str = "en", target_lang: str = "ko"):
    """ë²ˆì—­ ì˜ˆì‹œ"""
    try:
        translator = pipeline(
            "translation",
            model=f"Helsinki-NLP/opus-mt-{source_lang}-{target_lang}",
            token=config.HUGGINGFACE_API_KEY
        )
        result = translator(text)
        return result
    except Exception as e:
        print(f"âŒ ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def custom_model_example():
    """ì»¤ìŠ¤í…€ ëª¨ë¸ ì‚¬ìš© ì˜ˆì‹œ"""
    try:
        # í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
        model_name = config.HUGGINGFACE_MODEL
        tokenizer = AutoTokenizer.from_pretrained(model_name, token=config.HUGGINGFACE_API_KEY)
        model = AutoModel.from_pretrained(model_name, token=config.HUGGINGFACE_API_KEY)
        
        # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
        text = "Hello, how are you?"
        inputs = tokenizer(text, return_tensors="pt")
        
        # ëª¨ë¸ ì¶”ë¡ 
        outputs = model(**inputs)
        
        print(f"âœ… ì»¤ìŠ¤í…€ ëª¨ë¸ '{model_name}' ì‚¬ìš© ì„±ê³µ")
        print(f"   ì…ë ¥ í…ìŠ¤íŠ¸: {text}")
        print(f"   ì¶œë ¥ í˜•íƒœ: {outputs.last_hidden_state.shape}")
        
        return outputs
    except Exception as e:
        print(f"âŒ ì»¤ìŠ¤í…€ ëª¨ë¸ ì‚¬ìš© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

if __name__ == "__main__":
    # ì„¤ì • í™•ì¸
    config.print_config()
    
    # Hugging Face ì„¤ì •
    setup_huggingface()
    
    # ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    print("\nğŸ” ëª¨ë¸ íƒìƒ‰:")
    get_huggingface_models()
    
    # í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì˜ˆì‹œ
    print("\nğŸ“Š í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì˜ˆì‹œ:")
    classification_result = text_classification_example("I love this movie!")
    if classification_result:
        print(f"ë¶„ë¥˜ ê²°ê³¼: {classification_result}")
    
    # ê°ì • ë¶„ì„ ì˜ˆì‹œ
    print("\nğŸ˜Š ê°ì • ë¶„ì„ ì˜ˆì‹œ:")
    sentiment_result = sentiment_analysis_example("ì˜¤ëŠ˜ì€ ì •ë§ ì¢‹ì€ ë‚ ì”¨ë„¤ìš”!")
    if sentiment_result:
        print(f"ê°ì • ë¶„ì„ ê²°ê³¼: {sentiment_result}")
    
    # í…ìŠ¤íŠ¸ ìƒì„± ì˜ˆì‹œ
    print("\nâœï¸ í…ìŠ¤íŠ¸ ìƒì„± ì˜ˆì‹œ:")
    generation_result = text_generation_example("The future of artificial intelligence")
    if generation_result:
        print(f"ìƒì„±ëœ í…ìŠ¤íŠ¸: {generation_result[0]['generated_text']}")
    
    # ë²ˆì—­ ì˜ˆì‹œ
    print("\nğŸŒ ë²ˆì—­ ì˜ˆì‹œ:")
    translation_result = translation_example("Hello, how are you today?", "en", "ko")
    if translation_result:
        print(f"ë²ˆì—­ ê²°ê³¼: {translation_result[0]['translation_text']}")
    
    # ì»¤ìŠ¤í…€ ëª¨ë¸ ì˜ˆì‹œ
    print("\nğŸ”§ ì»¤ìŠ¤í…€ ëª¨ë¸ ì˜ˆì‹œ:")
    custom_result = custom_model_example() 